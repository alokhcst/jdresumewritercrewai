{
  "Personal Information": {
    "Name": "ALOK GUPTA",
    "Email": "alokgupta.mdi@gmail.com",
    "Phone": "+1 678 779 6079",
    "LinkedIn": "linkedin.com/in/alokgupta-mdi/",
    "GitHub": "https://github.com/alokhcst",
    "Location": "Atlanta, GA"
  },
  "Professional Summary": "Experienced Data Architect solutioning financial data products in Data Lake, Datawarehouse and Big Data technologies. Adept in design, development, deployment, and delivering scalable, efficient, and elegant data-driven financial solutions to extract insight for business decision-making.",
  "Technical Skills": {
    "BI Tools": [
      "Power BI",
      "Sigma Compute",
      "Looker",
      "Tableau",
      "SAP Business Objects",
      "Cognos"
    ],
    "ETL Tools": [
      "DBT",
      "Informatica Power Center",
      "Manager and Monitor",
      "AWS Glue",
      "Azure Data Factory",
      "SSIS"
    ],
    "Data Warehousing": [
      "Snowflake (Snowpark, Snowpipe, Snow Share, zero copy clone, Time Travel)",
      "Redshift",
      "Teradata",
      "BW HANA",
      "Hadoop Ecosystem (Hive, HDFS, Spark, Pig, Sqoop, Oozie, YARN)"
    ],
    "AWS Cloud Services": [
      "S3",
      "Lambda",
      "Step Functions",
      "EC2",
      "RDS",
      "Athena",
      "Glue",
      "EMR",
      "Data Pipeline",
      "AWS Lake Formation"
    ],
    "Azure Cloud Services": [
      "Power BI",
      "Azure Onelake",
      "Azure SQL Database",
      "Azure Databricks",
      "Data Fabrics",
      "Synapse Analytics",
      "Cosmos Database",
      "Azure Data Factory",
      "Azure Data Lake Storage Gen2",
      "Azure Functions",
      "Azure Logic Apps",
      "Azure Monitor"
    ],
    "Languages": [
      "SQL",
      "Python",
      "PySpark",
      "Scala",
      "PL/SQL",
      "Ksh/Bash",
      "Linux Shell Scripting"
    ],
    "Tools": [
      "Git",
      "Jenkins",
      "JIRA",
      "Confluence"
    ],
    "Methodologies": [
      "Agile",
      "Scrum",
      "CI/CD",
      "TDD",
      "Microservices",
      "RESTful APIs"
    ]
  },
  "Work Experience": [
    {
      "Company": "INVISTA (KOCH Industries), USA",
      "Title": "Senior Data Architect",
      "Duration": "November 2023 - Present",
      "Responsibilities": [
        "Led a $4.5 million technical migration on dbt (Data Build Tool), outlining project objectives and dependencies, defining transformation rules and migration procedures. Ensured data integrity, accuracy, completeness, and compliance with business requirements. Achieved a 35% improvement in data transformation time and established comprehensive data lineage.",
        "Solutioned and implemented data replication across multiple accounts using Snowflake data share for General Ledger anomaly detection, reducing financial anomalies by 85% and supporting near real-time reconciliation for the core finance team.",
        "Collaborated with finance stakeholders to assess their reporting needs and architected Financial Falcon Data products on Snowflake such as Ledger Detail, Trial Balance Quick, Plant Operating Costs, and SG&A Expenses. Enabled month-end finance closing in 5 days, saving 15 FTE working hours.",
        "Ingested real-time IoT sensor data into AWS Data Lake (S3) via Kinesis Data Streams and Firehose for a polymerization project, providing timely insights for business continuity and reducing environmental risks.",
        "Designed, developed, and managed a data lake ingestion framework utilizing AWS technologies, including AWS DMS, Glue, Athena, DynamoDB, S3, Lambda, EC2, EBS, and Event Bridge.",
        "Initiated FinOps, refactored existing data pipelines by modifying Python scripts and SQL query designs. Improved query performance in Snowflake using CTEs, query pruning, clustering, materialization, prevention of memory spills, and warehouse rightsizing, resulting in faster query response times and a 37% decrease in total cost of ownership ($300K yearly savings). Enabled delta data sharing, doubling report generation speed in Power BI.",
        "Spearheaded a Sigma tool POC, created and updated playbooks, and facilitated knowledge sharing. Integrated with Snowflake via oAuth for rapid prototyping and data profiling, and with IdentityNow for RBAC. Prototyped Vendor Invoice Management for cost center allocation and accrual adjustments. Built a Touch Analysis KPI that lowered payment failures and improved accounts payable closure rates.",
        "Drove the adoption of Data citizen reporting processes on Delta Data Lake supporting self-service ad-hoc reporting and rapid solution design, resulting in faster decision-making and more efficient data democratization for the capability team.",
        "Orchestrated the migration of legacy EBX data acquisition platform to new Event Driven architecture with configurable pipelines. This pub/sub model achieved 20% reduction in operational cost, ensured data consistency, and automated data acquisition and ingestion.",
        "Developed observability dashboards using DataDog that provided visibility of SLAs on data, product quality, data freshness, and other issues. Built trust in the business by sending proactive alerts in case of critical failure.",
        "Collaborated with stakeholders to create reporting-line transparency with frequent updates on project status. Brainstormed on schema effectiveness and ensured business needs were met while adhering to best practices for data management and Ralph Kimball dimensional modelling.",
        "Developed financial data models and data products in Snowflake adhering to best practices star schema dimensional modelling (Ralph Kimball). Ensured data quality and accessibility within MDM Tool Alation for data discovery, cataloging, and governance.",
        "Spearheaded end-to-end ETL pipeline delivery of a data platform project, leading requirements gathering, conceptual, logical, and physical system design to ensure scalability, agility, manageability, and cost effectiveness.",
        "Established comprehensive data security measures using Snowflake RBAC, data masking, and Datadog for monitoring, reducing vulnerability exploits by 40% within the first year.",
        "Developed a decision-making framework on emerging technologies and solutions. Collaborated with business leaders to build a perspective on new frameworks and technologies such as Generative AI, LLM, and Agent AI. Facilitated prototypes to influence strategic goals and targeted use cases.",
        "Developed a chatbot using a generative AI framework for data products, allowing business users to quickly find contextualized answers using retrieval augmented generation (RAG) for context with AmazonQ and AWS Bedrock services."
      ]
    },
    {
      "Company": "Georgia-Pacific (KOCH Industries), USA ",
      "Title": "Solutions Architect Leader",
      "Duration": "March 2021 - October 2023",
      "Responsibilities": [
        "Designed, developed, and optimized large-scale Azure Databricks ETL pipelines for wood and fiber sourcing customer analytics project, reducing data processing time by 40% and enabling real-time insights on wood chip quality and price determination.",
        "Spearheaded the rollout of DevSecOps new pipelines using Azure DevOps (ADO), standardizing CI/CD practices across BP and Packaging divisions, also using git for versioning of python code. This initiative enabled unit testing, agile debugging and troubleshooting for user acceptance testing, reduced deployment errors by 40%, thus accelerated release cycles.",
        "Conducted comprehensive evaluations of third-party SaaS offerings, using Vendor Software Evaluation Custom Tool developed to benchmark vendor solutions for security and compliance. Streamlined the BP and Packaging divisions application portfolio, resulting in a 20% reduction in legacy system maintenance costs and technical debt.",
        "Architected and implemented an Integration 2.0 strategy, defining API-led connectivity using AWS Eventbridge, Amazon API Gateway and AWS AppSync. This reduced integration complexity and improved time-to-market for new features by 25%.",
        "Partnered with application owners and data teams to design secure, compliant solutions, leveraging AWS Identity and Access Management (IAM), AWS KMS, and AWS Shield, enabling role-based access controls. Worked alongside Enterprise Architects and Solution Architects to create strategic roadmaps for Modern Solutions and integrate future-ready components, identifying and addressing technical debt to improve system agility.",
        "Facilitated architecture review workshops and developed reusable reference architectures for various business segments under BP and Packaging division, ensuring business alignment with AWS and industry best practices and business goals.",
        "Oversaw the Disaster Recovery program, implementing AWS Disaster Recovery and Commvault Backup and cross-region replication strategies to meet RTO/RPO targets. Optimized cloud resource utilization with AWS Cost Explorer and Trusted Advisor, achieving a 15% reduction in operational costs while enhancing security and performance."
      ]
    },
    {
      "Company": "NCR Corporation, USA",
      "Title": "Solution Architect",
      "Duration": "April 2011 - January 2021",
      "Responsibilities": [
        "Collaborated with vendors to evaluate frameworks like Hortonworks, Cloudera, Azure Synapse Analytics, HDInsight, and Databricks.",
        "Implemented data ingestion using Azure Kafka and Data Factory, migrating data to ADLS Gen2.",
        "Implemented Delta Lake as a key component of Databricks to provide a reliable storage layer that enables ACID transactions for better data governance, versioning, and scalable metadata management.",
        "Strong knowledge of Databricks Lakehouse architecture (Delta Lake, Apache Iceberg, Unity Catalog, ML) using RBAC supporting a Data Mesh framework.",
        "Extended on-prem platforms to cloud with Azure Synapse Analytics and SQL optimization.",
        "Led a team for integration strategy, providing end-to-end architecture design in Big Data and Enterprise Data Warehouse projects.",
        "Designed solutions for Oracle ERP data, applied dimensional modeling techniques, and engineered a Hadoop platform and data governance framework.",
        "Developed self-service analysis models with Tableau and Power BI."
      ]
    }
  ],
  "Education": [
    {
      "Degree": "Master's",
      "Field": "Information Technology",
      "Institution": "Management Development Institute",
      "Duration": "May 2011 - June 2013"
    },
    {
      "Degree": "Bachelor's",
      "Field": "Information Technology",
      "Institution": "Uttar Pradesh Technical University",
      "Duration": "May 2001 - June 2005"
    }
  ],
  "Certifications": [
    {
      "Title": "Advance RAG applications with Vector Databases",
      "Provider": "LinkedIn",
      "Date": "Jan 2025"
    },
    {
      "Title": "Neo4j-GraphDB",
      "Provider": "LinkedIn",
      "Date": "Mar 2024"
    },
    {
      "Title": "AWS Certified Solutions Architect - Associate SAA-C03",
      "Provider": "AWS",
      "Date": "Nov-2022"
    },
    {
      "Title": "Microsoft Certified: Azure Solutions Architect Expert - AZ-303 and AZ-304",
      "Provider": "Azure",
      "Date": "Apr-2021"
    },
    {
      "Title": "Microsoft Certified: Azure Fundamentals: AZ-900: H588-6014",
      "Provider": "Azure",
      "Date": "Dec-2020"
    }
  ],
  "Notable Achievements and Awards": [
    "Received 'Innovation Award' from KOCH Leadership in 2024 for implementing Data Mesh; this increased the focus of delivery across the capabilities and promoted LEAN concept by reducing delivery waste by 35%.",
    "Awarded 'Collaboration Partner' of the Year at AWS 2022 conference for using AWS Disaster recovery services and being first partner and adopter; this implementation increased the reliability of manufacturing and supply chain infrastructure and resulted in annual savings of $1.3 million.",
    "Awarded 'Above and Beyond' recognition for designing and delivering 'Quote to Cash' solution for NCR Financial Outlook, which increased forecasted revenue recognition accuracy by 90%."
  ],
  "Volunteer Experience": [
    {
      "Organization": "ATLANTA HABITAT",
      "Role": "Volunteer",
      "Description": "Frequently assist with various stages of home construction, including framing, siding, painting, and landscaping for needy people."
    },
    {
      "Organization": "GREEN CELL ATL",
      "Role": "Volunteer",
      "Description": "Promoting e-waste recycling and sustainability, participating in local events."
    }
  ]
}